"""
ReUnity Export and Portability Module

This module implements export-safe portability bundles with full provenance
tracking. Users can export their data in standardized formats for backup,
transfer, or use with other systems.

Features:
- Standardized export formats (JSON, encrypted bundles)
- Full provenance tracking (timestamps, versions, sources)
- Consent-aware exports (respects access scopes)
- Integrity verification (checksums, signatures)
- Import/restore functionality

DISCLAIMER: This is not a clinical or treatment document. It is a theoretical
and support framework only.

Author: Christopher Ezernack
"""

from __future__ import annotations

import hashlib
import json
import time
import uuid
import zipfile
from dataclasses import dataclass, field
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Any

from reunity.memory.continuity_store import (
    RecursiveIdentityMemoryEngine,
    ConsentScope,
)


# ============================================================================
# DISCLAIMER - Required in all exports
# ============================================================================
EXPORT_DISCLAIMER = """
================================================================================
REUNITY DATA EXPORT - IMPORTANT DISCLAIMER
================================================================================

This data export was generated by ReUnity, a trauma-aware AI support system.

IMPORTANT NOTICES:

1. NOT CLINICAL DATA: This is not a clinical or treatment document. ReUnity
   is a theoretical and support framework only. It is not intended to diagnose,
   treat, cure, or prevent any medical or psychological condition.

2. PERSONAL DATA: This export may contain sensitive personal information
   including emotional states, relationship patterns, and identity-related
   content. Handle with appropriate care and security.

3. CONSENT SCOPES: Data in this export respects the consent scopes set by
   the user. Some data may have been filtered based on access permissions.

4. PROVENANCE: This export includes provenance information to track the
   source, version, and timestamp of the data. This information should be
   preserved for data integrity.

5. PROFESSIONAL SUPPORT: If you are experiencing mental health difficulties,
   please consult with qualified mental health professionals. This data is
   meant to support, not replace, professional care.

CRISIS RESOURCES:
- National Suicide Prevention Lifeline: 988 (US)
- Crisis Text Line: Text HOME to 741741 (US)
- International Association for Suicide Prevention: https://www.iasp.info/

================================================================================
"""


@dataclass
class Provenance:
    """Provenance information for exported data."""

    export_id: str
    export_timestamp: float
    export_version: str
    system_name: str
    system_version: str
    source_identity: str | None
    consent_scope_used: str
    data_hash: str
    entry_count: int
    metadata: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "export_id": self.export_id,
            "export_timestamp": self.export_timestamp,
            "export_datetime": datetime.fromtimestamp(
                self.export_timestamp
            ).isoformat(),
            "export_version": self.export_version,
            "system_name": self.system_name,
            "system_version": self.system_version,
            "source_identity": self.source_identity,
            "consent_scope_used": self.consent_scope_used,
            "data_hash": self.data_hash,
            "entry_count": self.entry_count,
            "metadata": self.metadata,
        }


@dataclass
class ExportBundle:
    """A complete export bundle with data and provenance."""

    provenance: Provenance
    disclaimer: str
    memories: list[dict[str, Any]]
    journals: list[dict[str, Any]]
    timeline: list[dict[str, Any]]
    lattice_graph: dict[str, Any] | None
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class ImportResult:
    """Result of an import operation."""

    success: bool
    memories_imported: int
    journals_imported: int
    timeline_events_imported: int
    errors: list[str]
    warnings: list[str]
    provenance: Provenance | None


class PortabilityManager:
    """
    Manager for data export and import with provenance tracking.

    Provides standardized export formats with full provenance information,
    enabling users to maintain data sovereignty and portability.

    DISCLAIMER: This is not a clinical or treatment document. It is a
    theoretical and support framework only.
    """

    EXPORT_VERSION = "1.0.0"
    SYSTEM_NAME = "ReUnity"
    SYSTEM_VERSION = "1.0.0"

    def __init__(
        self,
        memory_engine: RecursiveIdentityMemoryEngine | None = None,
    ) -> None:
        """
        Initialize the portability manager.

        Args:
            memory_engine: Optional memory engine for direct exports.
        """
        self.memory_engine = memory_engine

    def create_export_bundle(
        self,
        identity: str | None = None,
        consent_scope: ConsentScope = ConsentScope.SELF_ONLY,
        include_memories: bool = True,
        include_journals: bool = True,
        include_timeline: bool = True,
        include_lattice: bool = False,
        lattice_data: dict[str, Any] | None = None,
        custom_metadata: dict[str, Any] | None = None,
    ) -> ExportBundle:
        """
        Create a complete export bundle.

        Args:
            identity: Filter by identity (None = all).
            consent_scope: Consent scope for access.
            include_memories: Include memories in export.
            include_journals: Include journals in export.
            include_timeline: Include timeline in export.
            include_lattice: Include lattice graph in export.
            lattice_data: Pre-exported lattice data.
            custom_metadata: Additional metadata to include.

        Returns:
            Complete ExportBundle.
        """
        memories = []
        journals = []
        timeline = []

        if self.memory_engine:
            # Export from memory engine
            export_data = self.memory_engine.export_memories(
                identity=identity,
                accessor_scope=consent_scope,
            )

            if include_memories:
                memories = export_data.get("memories", [])

            if include_journals:
                journals = export_data.get("journals", [])

            if include_timeline:
                timeline = export_data.get("timeline", [])

        # Calculate data hash for integrity
        data_for_hash = json.dumps({
            "memories": memories,
            "journals": journals,
            "timeline": timeline,
        }, sort_keys=True)
        data_hash = hashlib.sha256(data_for_hash.encode()).hexdigest()

        # Create provenance
        provenance = Provenance(
            export_id=str(uuid.uuid4()),
            export_timestamp=time.time(),
            export_version=self.EXPORT_VERSION,
            system_name=self.SYSTEM_NAME,
            system_version=self.SYSTEM_VERSION,
            source_identity=identity,
            consent_scope_used=consent_scope.value,
            data_hash=data_hash,
            entry_count=len(memories) + len(journals) + len(timeline),
            metadata=custom_metadata or {},
        )

        return ExportBundle(
            provenance=provenance,
            disclaimer=EXPORT_DISCLAIMER,
            memories=memories,
            journals=journals,
            timeline=timeline,
            lattice_graph=lattice_data if include_lattice else None,
            metadata=custom_metadata or {},
        )

    def export_to_json(
        self,
        bundle: ExportBundle,
        pretty: bool = True,
    ) -> str:
        """
        Export bundle to JSON string.

        Args:
            bundle: Export bundle to serialize.
            pretty: Use pretty formatting.

        Returns:
            JSON string.
        """
        export_dict = {
            "disclaimer": bundle.disclaimer,
            "provenance": bundle.provenance.to_dict(),
            "data": {
                "memories": bundle.memories,
                "journals": bundle.journals,
                "timeline": bundle.timeline,
            },
            "metadata": bundle.metadata,
        }

        if bundle.lattice_graph:
            export_dict["data"]["lattice_graph"] = bundle.lattice_graph

        if pretty:
            return json.dumps(export_dict, indent=2, default=str)
        return json.dumps(export_dict, default=str)

    def export_to_file(
        self,
        bundle: ExportBundle,
        file_path: Path,
        compress: bool = True,
    ) -> Path:
        """
        Export bundle to file.

        Args:
            bundle: Export bundle.
            file_path: Target file path.
            compress: Whether to compress the output.

        Returns:
            Path to created file.
        """
        json_data = self.export_to_json(bundle)

        if compress:
            # Create zip archive
            zip_path = file_path.with_suffix(".reunity.zip")
            with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
                zf.writestr("export.json", json_data)
                zf.writestr("DISCLAIMER.txt", EXPORT_DISCLAIMER)
                zf.writestr(
                    "provenance.json",
                    json.dumps(bundle.provenance.to_dict(), indent=2),
                )
            return zip_path
        else:
            # Write plain JSON
            json_path = file_path.with_suffix(".reunity.json")
            with open(json_path, "w") as f:
                f.write(json_data)
            return json_path

    def export_to_bytes(
        self,
        bundle: ExportBundle,
        compress: bool = True,
    ) -> bytes:
        """
        Export bundle to bytes.

        Args:
            bundle: Export bundle.
            compress: Whether to compress.

        Returns:
            Bytes of export data.
        """
        json_data = self.export_to_json(bundle)

        if compress:
            buffer = BytesIO()
            with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                zf.writestr("export.json", json_data)
                zf.writestr("DISCLAIMER.txt", EXPORT_DISCLAIMER)
                zf.writestr(
                    "provenance.json",
                    json.dumps(bundle.provenance.to_dict(), indent=2),
                )
            return buffer.getvalue()
        else:
            return json_data.encode("utf-8")

    def import_from_json(
        self,
        json_data: str,
        verify_integrity: bool = True,
    ) -> ImportResult:
        """
        Import data from JSON string.

        Args:
            json_data: JSON string to import.
            verify_integrity: Whether to verify data hash.

        Returns:
            ImportResult with details.
        """
        errors = []
        warnings = []

        try:
            data = json.loads(json_data)
        except json.JSONDecodeError as e:
            return ImportResult(
                success=False,
                memories_imported=0,
                journals_imported=0,
                timeline_events_imported=0,
                errors=[f"Invalid JSON: {e}"],
                warnings=[],
                provenance=None,
            )

        # Extract provenance
        prov_data = data.get("provenance", {})
        provenance = Provenance(
            export_id=prov_data.get("export_id", "unknown"),
            export_timestamp=prov_data.get("export_timestamp", 0),
            export_version=prov_data.get("export_version", "unknown"),
            system_name=prov_data.get("system_name", "unknown"),
            system_version=prov_data.get("system_version", "unknown"),
            source_identity=prov_data.get("source_identity"),
            consent_scope_used=prov_data.get("consent_scope_used", "private"),
            data_hash=prov_data.get("data_hash", ""),
            entry_count=prov_data.get("entry_count", 0),
        )

        # Verify integrity if requested
        if verify_integrity and provenance.data_hash:
            export_data = data.get("data", {})
            data_for_hash = json.dumps({
                "memories": export_data.get("memories", []),
                "journals": export_data.get("journals", []),
                "timeline": export_data.get("timeline", []),
            }, sort_keys=True)
            calculated_hash = hashlib.sha256(data_for_hash.encode()).hexdigest()

            if calculated_hash != provenance.data_hash:
                warnings.append(
                    "Data hash mismatch - data may have been modified"
                )

        # Extract data
        export_data = data.get("data", {})
        memories = export_data.get("memories", [])
        journals = export_data.get("journals", [])
        timeline = export_data.get("timeline", [])

        # Import to memory engine if available
        memories_imported = 0
        journals_imported = 0
        timeline_imported = 0

        if self.memory_engine:
            memories_imported = self.memory_engine.import_memories({
                "memories": memories,
            })

        return ImportResult(
            success=len(errors) == 0,
            memories_imported=memories_imported,
            journals_imported=len(journals),
            timeline_events_imported=len(timeline),
            errors=errors,
            warnings=warnings,
            provenance=provenance,
        )

    def import_from_file(
        self,
        file_path: Path,
        verify_integrity: bool = True,
    ) -> ImportResult:
        """
        Import data from file.

        Args:
            file_path: Path to import file.
            verify_integrity: Whether to verify data hash.

        Returns:
            ImportResult with details.
        """
        if file_path.suffix == ".zip":
            # Extract from zip
            with zipfile.ZipFile(file_path, "r") as zf:
                json_data = zf.read("export.json").decode("utf-8")
        else:
            # Read plain JSON
            with open(file_path, "r") as f:
                json_data = f.read()

        return self.import_from_json(json_data, verify_integrity)

    def import_from_bytes(
        self,
        data: bytes,
        compressed: bool = True,
        verify_integrity: bool = True,
    ) -> ImportResult:
        """
        Import data from bytes.

        Args:
            data: Bytes to import.
            compressed: Whether data is compressed.
            verify_integrity: Whether to verify data hash.

        Returns:
            ImportResult with details.
        """
        if compressed:
            buffer = BytesIO(data)
            with zipfile.ZipFile(buffer, "r") as zf:
                json_data = zf.read("export.json").decode("utf-8")
        else:
            json_data = data.decode("utf-8")

        return self.import_from_json(json_data, verify_integrity)

    def verify_export_integrity(
        self,
        json_data: str,
    ) -> tuple[bool, str]:
        """
        Verify integrity of export data.

        Args:
            json_data: JSON string to verify.

        Returns:
            Tuple of (is_valid, message).
        """
        try:
            data = json.loads(json_data)
        except json.JSONDecodeError:
            return False, "Invalid JSON format"

        prov_data = data.get("provenance", {})
        stored_hash = prov_data.get("data_hash", "")

        if not stored_hash:
            return False, "No data hash found in provenance"

        export_data = data.get("data", {})
        data_for_hash = json.dumps({
            "memories": export_data.get("memories", []),
            "journals": export_data.get("journals", []),
            "timeline": export_data.get("timeline", []),
        }, sort_keys=True)
        calculated_hash = hashlib.sha256(data_for_hash.encode()).hexdigest()

        if calculated_hash == stored_hash:
            return True, "Data integrity verified"
        else:
            return False, "Data hash mismatch - data may have been modified"

    def create_minimal_export(
        self,
        memories: list[dict[str, Any]],
        identity: str | None = None,
    ) -> dict[str, Any]:
        """
        Create a minimal export without full bundle structure.

        Useful for quick exports or API responses.

        Args:
            memories: List of memory dictionaries.
            identity: Source identity.

        Returns:
            Minimal export dictionary.
        """
        data_for_hash = json.dumps(memories, sort_keys=True)
        data_hash = hashlib.sha256(data_for_hash.encode()).hexdigest()

        return {
            "disclaimer": "ReUnity export - not clinical data",
            "provenance": {
                "export_id": str(uuid.uuid4()),
                "export_timestamp": time.time(),
                "system": self.SYSTEM_NAME,
                "version": self.SYSTEM_VERSION,
                "source_identity": identity,
                "data_hash": data_hash,
                "entry_count": len(memories),
            },
            "memories": memories,
        }


def get_disclaimer() -> str:
    """Get the export disclaimer text."""
    return EXPORT_DISCLAIMER


def create_provenance(
    data: dict[str, Any],
    identity: str | None = None,
    metadata: dict[str, Any] | None = None,
) -> Provenance:
    """
    Create provenance for arbitrary data.

    Args:
        data: Data to create provenance for.
        identity: Source identity.
        metadata: Additional metadata.

    Returns:
        Provenance object.
    """
    data_str = json.dumps(data, sort_keys=True, default=str)
    data_hash = hashlib.sha256(data_str.encode()).hexdigest()

    return Provenance(
        export_id=str(uuid.uuid4()),
        export_timestamp=time.time(),
        export_version="1.0.0",
        system_name="ReUnity",
        system_version="1.0.0",
        source_identity=identity,
        consent_scope_used="self_only",
        data_hash=data_hash,
        entry_count=len(data) if isinstance(data, (list, dict)) else 1,
        metadata=metadata or {},
    )
